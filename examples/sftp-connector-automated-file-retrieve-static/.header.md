# AWS Transfer Family SFTP Connector Example with Automated File Retrieval

This example demonstrates how to use the AWS Transfer Family SFTP connector module to automatically retrieve specific files from an external SFTP server on a scheduled basis using EventBridge Scheduler.

## Architecture

This example creates:

1. An AWS Transfer Family SFTP connector that connects to an external SFTP server
2. An S3 bucket for storing retrieved files with KMS encryption
3. A KMS key for encryption of all resources
4. An EventBridge Scheduler that automatically triggers file retrieval on a configurable schedule
5. Optional DynamoDB table for tracking file transfer status and metadata
6. IAM roles and policies for secure access between services

## How It Works

1. EventBridge Scheduler runs on the configured schedule (e.g., hourly, daily)
2. The scheduler directly calls the AWS Transfer Family StartFileTransfer API using the configured IAM role
3. The connector retrieves the specified files from the external SFTP server
4. Files are automatically stored in the S3 bucket with the configured prefix
5. Optional DynamoDB logging tracks transfer status and metadata

## SFTP Credentials

This example provides two options for SFTP credentials:

1. **Use an existing Secrets Manager secret** - Provide the ARN of an existing secret containing SFTP credentials
2. **Create a new secret** - Provide username and private key to create a new secret

### Existing Secret Format

If using an existing secret, it must contain credentials in this format:

```json
{
  "Username": "your-sftp-username",
  "PrivateKey": "begin pk"
}
```

## Usage

There are several ways to provide the required variables:

### Option 1: Using terraform.tfvars (Recommended)

Edit the `terraform.tfvars` file with your specific values:

```hcl
aws_region = "us-east-1"
sftp_server_endpoint = "example.com"
# Leave existing_secret_arn empty to create a new secret
existing_secret_arn = ""
sftp_username = "sftp-user"
sftp_private_key = "begin pk"
trusted_host_keys = ["ssh-rsa AAAAB3NzaC1yc2EAAAA..."]
file_paths_to_retrieve = ["/uploads/report.csv", "/data/file1.txt"]
eventbridge_schedule = "rate(1 hour)"
s3_prefix = "retrieved-files"
enable_dynamodb_tracking = false
```

Then simply run:

```bash
terraform init
terraform apply
```

### Option 2: Using Environment Variables

Set the required environment variables:

```bash
export TF_VAR_sftp_server_endpoint="example.com"
# Either provide an existing secret ARN
export TF_VAR_existing_secret_arn="arn:aws:secretsmanager:region:account-id:secret:secret-name"
# Or provide credentials to create a new secret
export TF_VAR_sftp_username="your-username"
export TF_VAR_sftp_private_key="$(cat ~/.ssh/id_rsa)"
export TF_VAR_trusted_host_keys='["ssh-rsa AAAAB3NzaC1yc2EAAAA..."]'
export TF_VAR_file_paths_to_retrieve='["/uploads/report.csv", "/data/file1.txt"]'
export TF_VAR_eventbridge_schedule="rate(1 hour)"
export TF_VAR_aws_region="us-east-1"

terraform init
terraform apply
```

### Option 3: Command Line Variables

```bash
terraform init
terraform apply -var="sftp_server_endpoint=example.com" \
                -var="sftp_username=your-username" \
                -var="sftp_private_key=$(cat ~/.ssh/id_rsa)" \
                -var='trusted_host_keys=["ssh-rsa AAAAB3NzaC1yc2EAAAA..."]' \
                -var='file_paths_to_retrieve=["/uploads/report.csv"]' \
                -var="eventbridge_schedule=rate(1 hour)" \
                -var="aws_region=us-east-1"
```

## Testing the Integration

After deploying the infrastructure, you can test the automatic file retrieval:

### Option 1: Wait for the scheduled retrieval

The EventBridge Scheduler will automatically trigger file retrieval based on your configured schedule.

### Option 2: Manually trigger the scheduler

```bash
# Get the scheduler name from Terraform outputs
terraform output scheduler_name

# Manually trigger the scheduler (if supported by AWS CLI)
aws scheduler invoke-schedule --name $(terraform output -raw scheduler_name)
```

### Option 3: Check the S3 bucket for retrieved files

```bash
# Get the S3 bucket name from Terraform outputs
terraform output s3_bucket_name

# List retrieved files
aws s3 ls s3://$(terraform output -raw s3_bucket_name)/retrieved-files/
```

### Monitoring the Retrieval

You can monitor the retrieval process by checking:

1. **Transfer Family console** to see transfer history and connector status

2. **S3 bucket** to verify files are being retrieved and stored

3. **EventBridge Scheduler console** to see schedule execution history

4. **DynamoDB table** (if enabled) to track transfer metadata:

   ```bash
   # Get the DynamoDB table name from Terraform outputs (if enabled)
   terraform output dynamodb_table_name
   
   # Query transfer records
   aws dynamodb scan --table-name $(terraform output -raw dynamodb_table_name)
   ```
